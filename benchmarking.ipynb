{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "463ee857",
   "metadata": {},
   "source": [
    "# Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e4aa311",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mirdata\n",
    "from pesto import load_model\n",
    "import torch\n",
    "import mir_eval\n",
    "import pesto\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898df4ef",
   "metadata": {},
   "source": [
    "# Benchmarking on MDB-stem-synth with PESTO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baa8a4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = mirdata.initialize(\"mdb_stem_synth\")\n",
    "# dataset.download() #this line has to be commented and re-run if the dataset is not already installed\n",
    "# dataset.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3816e476",
   "metadata": {},
   "source": [
    "### Single track (test) benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66cde33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ---- dataset / track ----\n",
    "track_id = \"AClassicEducation_NightOwl_STEM_01\"\n",
    "track = dataset.track(track_id)  # assumes `dataset` already exists\n",
    "audio, sr = track.audio  # mirdata returns (samples, channels) usually\n",
    "\n",
    "# ---- mono + torch tensor (PESTO guideline) ----\n",
    "audio_mono = audio.mean(axis=-1) if audio.ndim > 1 else audio\n",
    "x = torch.from_numpy(audio_mono).float()  # (num_samples,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4a81ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- device + model (load once) ----\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "step_size_ms = 20.0\n",
    "\n",
    "# Important: pass sampling_rate to match your data (repo shows this pattern in advanced usage)\n",
    "pesto_model = load_model(\"mir-1k_g7\", step_size=step_size_ms, sampling_rate=sr).to(device)\n",
    "pesto_model.eval()\n",
    "\n",
    "# ---- inference ----\n",
    "with torch.no_grad():\n",
    "    # Repo example uses: f0, conf, amp = f0_estimator(x, convert_to_freq=True, return_activations=False)\n",
    "    f0, conf, amp = pesto_model(\n",
    "        x.to(device),\n",
    "        convert_to_freq=True,\n",
    "        return_activations=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ace1589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Results for Track: AClassicEducation_NightOwl_STEM_01 ---\n",
      "Raw Pitch Accuracy (RPA): 0.8691\n",
      "Raw Chroma Accuracy (RCA): 0.9100\n"
     ]
    }
   ],
   "source": [
    "# ---- move to numpy and build timestamps (repo: step_size in ms; hop is derived from it) ----\n",
    "f0_pred = f0.detach().cpu().numpy().squeeze()\n",
    "times_pred = np.arange(f0_pred.shape[-1]) * (step_size_ms / 1000.0)\n",
    "\n",
    "# mir_eval expects unvoiced = 0 Hz (not NaN)\n",
    "f0_pred = np.nan_to_num(f0_pred, nan=0.0)\n",
    "\n",
    "# ---- reference ----\n",
    "ref_times = track.f0.times\n",
    "ref_freqs = track.f0.frequencies\n",
    "\n",
    "# ---- metrics ----\n",
    "scores = mir_eval.melody.evaluate(ref_times, ref_freqs, times_pred, f0_pred)\n",
    "\n",
    "print(f\"--- Results for Track: {track_id} ---\")\n",
    "print(f\"Raw Pitch Accuracy (RPA): {scores['Raw Pitch Accuracy']:.4f}\")\n",
    "print(f\"Raw Chroma Accuracy (RCA): {scores['Raw Chroma Accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6415898",
   "metadata": {},
   "source": [
    "### Full dataset bechmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26820674",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating tracks: 100%|██████████| 230/230 [17:18<00:00,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores on MDB with PESTO:  {'Overall Accuracy': np.float64(0.39504905628006476), 'Raw Pitch Accuracy': np.float64(0.9098419948224401), 'Raw Chroma Accuracy': np.float64(0.9358192913900837), 'Voicing Recall': np.float64(0.9999996735270973), 'Voicing False Alarm': np.float64(0.9998373050855409)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "step_size_ms = 20.0\n",
    "\n",
    "metrics = [\n",
    "    \"Overall Accuracy\",\n",
    "    \"Raw Pitch Accuracy\",\n",
    "    \"Raw Chroma Accuracy\",\n",
    "    \"Voicing Recall\",\n",
    "    \"Voicing False Alarm\",\n",
    "]\n",
    "\n",
    "# load once (reload only if sr changes)\n",
    "track0 = dataset.track(dataset.track_ids[0])\n",
    "audio0, sr0 = track0.audio\n",
    "\n",
    "pesto_model = load_model(\"mir-1k_g7\", step_size=step_size_ms, sampling_rate=sr0).to(device).eval()\n",
    "\n",
    "sum_w = 0.0\n",
    "sum_scores = {m: 0.0 for m in metrics}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for track_id in tqdm(dataset.track_ids, desc=\"Evaluating tracks\"):\n",
    "        track = dataset.track(track_id)\n",
    "\n",
    "        audio, sr = track.audio\n",
    "        if sr != sr0:\n",
    "            sr0 = sr\n",
    "            pesto_model = load_model(\"mir-1k_g7\", step_size=step_size_ms, sampling_rate=sr0).to(device).eval()\n",
    "\n",
    "        audio_mono = audio.mean(axis=-1) if audio.ndim > 1 else audio\n",
    "        x = torch.from_numpy(audio_mono).float().to(device)\n",
    "\n",
    "        f0, conf, amp = pesto_model(x, convert_to_freq=True, return_activations=False)\n",
    "\n",
    "        f0_pred = f0.detach().cpu().numpy().squeeze()\n",
    "        f0_pred = np.nan_to_num(f0_pred, nan=0.0)\n",
    "        times_pred = np.arange(f0_pred.shape[-1]) * (step_size_ms / 1000.0)\n",
    "\n",
    "        ref_times = track.f0.times\n",
    "        ref_freqs = track.f0.frequencies\n",
    "\n",
    "        scores = mir_eval.melody.evaluate(ref_times, ref_freqs, times_pred, f0_pred)\n",
    "\n",
    "        w = float(ref_times.shape[0])\n",
    "        sum_w += w\n",
    "        for m in metrics:\n",
    "            sum_scores[m] += scores[m] * w\n",
    "\n",
    "final_scores = {m: (sum_scores[m] / sum_w) for m in metrics}\n",
    "print(\"Scores on MDB with PESTO: \",final_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6679fd58",
   "metadata": {},
   "source": [
    "# Benchmarking on Orchset with PESTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5453f9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 59.39it/s]\n",
      "100%|██████████| 64/64 [00:02<00:00, 31.10it/s]\n",
      "WARNING: Success: the dataset is complete and all files are valid.\n",
      "WARNING: --------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'metadata': {}, 'tracks': {}}, {'metadata': {}, 'tracks': {}})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = mirdata.initialize(\"orchset\")\n",
    "#dataset.download()\n",
    "#dataset.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9233509",
   "metadata": {},
   "source": [
    "### Single track (test) benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654d3849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- orchset / Beethoven-S3-I-ex1 ---\n",
      "RPA: 0.0368\n",
      "RCA: 0.6468\n"
     ]
    }
   ],
   "source": [
    "# orchset\n",
    "track_id = dataset.track_ids[0]\n",
    "track = dataset.track(track_id)\n",
    "\n",
    "# --- audio (orchset-specific) ---\n",
    "audio, sr = track.audio_mono  # <-- correct for Orchset :contentReference[oaicite:1]{index=1}\n",
    "x = torch.from_numpy(audio).float()\n",
    "\n",
    "# --- pesto ---\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "step_size_ms = 20.0\n",
    "model = load_model(\"mir-1k_g7\", step_size=step_size_ms, sampling_rate=sr).to(device).eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    f0, conf, amp = model(x.to(device), convert_to_freq=True, return_activations=False)\n",
    "\n",
    "f0_pred = f0.detach().cpu().numpy().squeeze()\n",
    "times_pred = np.arange(f0_pred.shape[-1]) * (step_size_ms / 1000.0)\n",
    "f0_pred = np.nan_to_num(f0_pred, nan=0.0)\n",
    "\n",
    "# --- reference (orchset-specific) ---\n",
    "ref_times = track.melody.times\n",
    "ref_freqs = track.melody.frequencies\n",
    "\n",
    "scores = mir_eval.melody.evaluate(ref_times, ref_freqs, times_pred, f0_pred)\n",
    "print(f\"--- orchset / {track_id} ---\")\n",
    "print(f\"RPA: {scores['Raw Pitch Accuracy']:.4f}\")\n",
    "print(f\"RCA: {scores['Raw Chroma Accuracy']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ae3161",
   "metadata": {},
   "source": [
    "### Full dataset Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ded56231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall score for PESTO on orchestra:  {'Overall Accuracy': np.float64(0.17283344364498154), 'Raw Pitch Accuracy': np.float64(0.18264329344654404), 'Raw Chroma Accuracy': np.float64(0.482965021786439), 'Voicing Recall': np.float64(0.9999686196039806), 'Voicing False Alarm': np.float64(0.9999546722490121)}\n"
     ]
    }
   ],
   "source": [
    "dataset = mirdata.initialize(\"orchset\")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "step_size_ms = 20.0\n",
    "\n",
    "metrics = [\n",
    "    \"Overall Accuracy\",\n",
    "    \"Raw Pitch Accuracy\",\n",
    "    \"Raw Chroma Accuracy\",\n",
    "    \"Voicing Recall\",\n",
    "    \"Voicing False Alarm\",\n",
    "]\n",
    "\n",
    "# load once (orchset has a fixed sr in practice, but keep this robust)\n",
    "track0 = dataset.track(dataset.track_ids[0])\n",
    "audio0, sr0 = track0.audio_mono\n",
    "model = load_model(\"mir-1k_g7\", step_size=step_size_ms, sampling_rate=sr0).to(device).eval()\n",
    "\n",
    "sum_w = 0.0\n",
    "sum_scores = {m: 0.0 for m in metrics}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for track_id in dataset.track_ids:\n",
    "        track = dataset.track(track_id)\n",
    "\n",
    "        audio, sr = track.audio_mono\n",
    "        if sr != sr0:\n",
    "            sr0 = sr\n",
    "            model = load_model(\"mir-1k_g7\", step_size=step_size_ms, sampling_rate=sr0).to(device).eval()\n",
    "\n",
    "        x = torch.from_numpy(audio).float().to(device)\n",
    "\n",
    "        f0, conf, amp = model(x, convert_to_freq=True, return_activations=False)\n",
    "        f0_pred = f0.detach().cpu().numpy().squeeze()\n",
    "        f0_pred = np.nan_to_num(f0_pred, nan=0.0)\n",
    "        times_pred = np.arange(f0_pred.shape[-1]) * (step_size_ms / 1000.0)\n",
    "\n",
    "        ref_times = track.melody.times\n",
    "        ref_freqs = track.melody.frequencies\n",
    "\n",
    "        scores = mir_eval.melody.evaluate(ref_times, ref_freqs, times_pred, f0_pred)\n",
    "\n",
    "        w = float(ref_times.shape[0])  # frame-weight by GT frames\n",
    "        sum_w += w\n",
    "        for m in metrics:\n",
    "            sum_scores[m] += scores[m] * w\n",
    "\n",
    "final_scores = {m: (sum_scores[m] / sum_w) for m in metrics}\n",
    "print(\"Overall score for PESTO on orchestra: \",final_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413af4be",
   "metadata": {},
   "source": [
    "# Benchmarking on vocadito with PESTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a91f82c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = mirdata.initialize(\"vocadito\")\n",
    "# dataset.download()  # vocadito is downloadable from Zenodo :contentReference[oaicite:4]{index=4}\n",
    "# dataset.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c0e71c",
   "metadata": {},
   "source": [
    "### Single track (test) benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0545b7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.9884678747940692 0.9884678747940692\n"
     ]
    }
   ],
   "source": [
    "# ---- pick a track ----\n",
    "track_id = dataset.track_ids[0]\n",
    "track = dataset.track(track_id)\n",
    "\n",
    "audio, sr = track.audio\n",
    "\n",
    "# ---- mono ----\n",
    "audio_mono = audio\n",
    "\n",
    "x = torch.from_numpy(audio_mono).float()\n",
    "\n",
    "# ---- model ----\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "step_size_ms = 20.0\n",
    "pesto_model = load_model(\"mir-1k_g7\", step_size=step_size_ms, sampling_rate=sr).to(device)\n",
    "pesto_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    f0, conf, amp = pesto_model(x.to(device), convert_to_freq=True, return_activations=False)\n",
    "\n",
    "f0_pred = f0.detach().cpu().numpy().squeeze()\n",
    "times_pred = np.arange(f0_pred.shape[-1]) * (step_size_ms / 1000.0)\n",
    "f0_pred = np.nan_to_num(f0_pred, nan=0.0)\n",
    "\n",
    "# ---- reference ----\n",
    "ref_times = track.f0.times\n",
    "ref_freqs = track.f0.frequencies\n",
    "\n",
    "scores = mir_eval.melody.evaluate(ref_times, ref_freqs, times_pred, f0_pred)\n",
    "print(track_id, scores[\"Raw Pitch Accuracy\"], scores[\"Raw Chroma Accuracy\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753cc5b2",
   "metadata": {},
   "source": [
    "### Full-data set benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbecf9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores on Vocadito with PESTO:  {'Overall Accuracy': np.float64(0.647838502081232), 'Raw Pitch Accuracy': np.float64(0.9811545604137208), 'Raw Chroma Accuracy': np.float64(0.9848056886964734), 'Voicing Recall': np.float64(0.9999869620011912), 'Voicing False Alarm': np.float64(0.9993370197333225)}\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "step_size_ms = 20.0\n",
    "\n",
    "# model loaded once\n",
    "track0 = dataset.track(dataset.track_ids[0])\n",
    "_, sr0 = track0.audio\n",
    "pesto_model = load_model(\"mir-1k_g7\", step_size=step_size_ms, sampling_rate=sr0).to(device)\n",
    "pesto_model.eval()\n",
    "\n",
    "metrics = [\n",
    "    \"Overall Accuracy\",\n",
    "    \"Raw Pitch Accuracy\",\n",
    "    \"Raw Chroma Accuracy\",\n",
    "    \"Voicing Recall\",\n",
    "    \"Voicing False Alarm\",\n",
    "]\n",
    "\n",
    "sum_w = 0.0\n",
    "sum_scores = {m: 0.0 for m in metrics}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for track_id in dataset.track_ids:\n",
    "        track = dataset.track(track_id)\n",
    "        audio, sr = track.audio\n",
    "\n",
    "        # if sampling rates differ across tracks, reload model to match sr\n",
    "        if sr != sr0:\n",
    "            sr0 = sr\n",
    "            pesto_model = load_model(\"mir-1k_g7\", step_size=step_size_ms, sampling_rate=sr0).to(device)\n",
    "            pesto_model.eval()\n",
    "\n",
    "        audio_mono = audio.mean(axis=-1) if getattr(audio, \"ndim\", 1) > 1 else audio\n",
    "        x = torch.from_numpy(audio_mono).float().to(device)\n",
    "\n",
    "        f0, conf, amp = pesto_model(x, convert_to_freq=True, return_activations=False)\n",
    "        f0_pred = f0.detach().cpu().numpy().squeeze()\n",
    "        f0_pred = np.nan_to_num(f0_pred, nan=0.0)\n",
    "\n",
    "        times_pred = np.arange(f0_pred.shape[-1]) * (step_size_ms / 1000.0)\n",
    "\n",
    "        ref_times = track.f0.times\n",
    "        ref_freqs = track.f0.frequencies\n",
    "\n",
    "        scores = mir_eval.melody.evaluate(ref_times, ref_freqs, times_pred, f0_pred)\n",
    "\n",
    "        w = float(ref_times.shape[0])  # frame-weighting by number of GT frames\n",
    "        sum_w += w\n",
    "        for m in metrics:\n",
    "            sum_scores[m] += scores[m] * w\n",
    "\n",
    "final_scores = {m: (sum_scores[m] / sum_w) for m in metrics}\n",
    "print(\"Scores on Vocadito with PESTO: \",final_scores)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "theorie_deep_learning",
   "language": "python",
   "name": "theorie_deep_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
